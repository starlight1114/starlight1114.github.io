[
  {
    "objectID": "Text_analysis.html",
    "href": "Text_analysis.html",
    "title": "Text Analysis",
    "section": "",
    "text": "Show the code\n#Create dataframe from the .txt file\nLA_bus_stops &lt;- read.csv(\"stops.txt\")\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\n\n\nShow the code\n#Count the length of the stop names, and extract each separate street from each stop name\nLA_bus_stops&lt;-LA_bus_stops |&gt; \n  mutate(stop_name_length = str_length(stop_name), \n         stop_street_1 = str_extract(stop_name, \".+(?= /)\"), \n         stop_street_2 = str_extract(stop_name, \"(?&lt;= / ).+\"))|&gt;\n    select(stop_id, stop_code, stop_name, stop_street_1, stop_street_2, stop_name_length, stop_lat, stop_lon)\n\n\n\n\nShow the code\n#Make a plot of how common each stop name length is\nLA_bus_stops|&gt;\n  ggplot(aes(x = stop_name_length))+\n  geom_histogram(binwidth = 0.5)+\n  labs(\n    x = \"Amount of characters in bus stop name\",\n    y = \"Count\",\n    title = \"What is the most common length of bus stop name?\"\n  )\n\n\n\n\n\n\n\n\n\nThis graph was made by counting the number of characters in each bus stop name and then plotting the number of times each amount of characters appeared in the dataset. The most common length of a bus stop name (which is usually formatted like “Paramount / Slauson”) is around 17 characters. Very few bus stops have less than 10 characters in them.\nThis is a table of the 10 streets with the most bus stops on them:\n\n\nShow the code\n#Count how many stops there are on each street\nCommon_streets1 &lt;- LA_bus_stops |&gt;\n  group_by(stop_street_1)|&gt;\n  summarize(count1 = n())|&gt;\n  rename(stop_street = stop_street_1)\n\nCommon_streets2 &lt;- LA_bus_stops |&gt;\n  group_by(stop_street_2)|&gt;\n  summarize(count2 = n())|&gt;\n  rename(stop_street = stop_street_2)\n\nCommon_streets &lt;- \n  full_join(Common_streets1, Common_streets2, by = \"stop_street\")|&gt;\n  mutate(total_count = (count1 + count2))|&gt;\n  select(stop_street, total_count)|&gt;\n  arrange(desc(total_count))|&gt;\n  filter(!is.na(stop_street))\n  \nprint(head(Common_streets,10))\n\n\n# A tibble: 10 × 2\n   stop_street total_count\n   &lt;chr&gt;             &lt;int&gt;\n 1 Vermont             255\n 2 Sunset              244\n 3 Broadway            232\n 4 Figueroa            224\n 5 Western             209\n 6 Victory             203\n 7 Olympic             201\n 8 Slauson             200\n 9 Imperial            192\n10 Rosecrans           186\n\n\nThis table was made by separating the bus stops’ names (which are usually formatted like “Paramount / Slauson”) into the first street (Paramount) and the second street (Slauson). The first street in this naming convention is the street that the bus runs on, and the second street is the cross street. After separating and counting the number of times each street is mentioned, I added the number of times the first street (Paramount in this example) is mentioned to the times it is mentioned as a cross street to get the number of bus stops that are on the street or stop at it as a cross street. stop_street tells you the street and total_count tells you the number of bus stops on or at that street.\nThis is a table of the 10 numbered streets with the most bus stops on them:\n\n\nShow the code\n#Which are the numbered streets with the most bus stops on them?\nCommon_streets|&gt;\n  filter(str_detect(stop_street, \"\\\\d.*\"))|&gt;\n  head(10)\n\n\n# A tibble: 10 × 2\n   stop_street total_count\n   &lt;chr&gt;             &lt;int&gt;\n 1 3rd                 131\n 2 7th                 118\n 3 6th                 110\n 4 8th                 104\n 5 1st                  99\n 6 4th                  54\n 7 5th                  46\n 8 9th                  32\n 9 23rd                 30\n10 120th                28\n\n\nThis table was created in the same way as the table above and then filtered to only count the numbered streets.\nThis analysis comes from publicly-available GTFS data provided by LA Metro.\n\nData Source: https://gitlab.com/LACMTA/gtfs_bus/-/tree/weekly-updated-service\nData Authors: https://developer.metro.net/gtfs-schedule-data/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is a site with a lot of things from DS002R."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS2 Project",
    "section": "",
    "text": "This is my website that is for Foundations of Data Science. Here is some basic information about me. I am a student that is doing Foundations of Data Science. This is something I made in my art class in Adobe Illustrator."
  },
  {
    "objectID": "Probability.html",
    "href": "Probability.html",
    "title": "Probability",
    "section": "",
    "text": "This project is inspired by a walk I took on last Saturday night. That night, I chose to walk around the 5Cs for around 3 hours, and I ran into around 8 of my close friends and had a lot of fun. As I walked, I ran into different sized groups of people, sometimes I would pass no one and sometimes I would pass huge groups of people.\nI chose to model the group sizes using an exponentially distributed set of random numbers with a rate of 0.25 since I approximate that I would run into an average of 4 people per minute.\n\n\nShow the code\n#sample plot of the distributions of the number of people I would see each minute over 3 hours\nset.seed(14)\nexprate &lt;- 0.25\nrandom_exponent &lt;- data.frame(n = map_dbl(1:180, function(x){\n    floor(rexp(1, exprate))}))\nggplot(random_exponent, aes(x = n))+\n  geom_histogram(binwidth = 0.5)+\n  labs(\n    x = \"Number of people seen each minute\",\n    title = \"Sample distribution of the number of people seen each minute for \\n180 minutes\"\n  )\n\n\n\n\n\n\n\n\n\nThis plot shows how many times I see a certain amount of people each minute if I walk around for 180 minutes. It shows that in this case, I would most commonly see 0 people per minute during my walk, followed by 1 people per minute, then 2 people per minute.\nEach person in each group of people I pass gets assigned a random number which indicates their closeness to me. I say that I am equally likely to see anyone from my closest friend (1) to my least close friend at the 5Cs (6000). For example, if I passed 2 people in the same minute (the randomly generated exponentially distributed number from before), my 5th closest friend and my 16th closest friend, that minute would be given the vector (5, 16). Then, I take the random numbers that have been generated for each minute and put them into one long vector, basically the vector of people I have seen throughout the entire night.\n\n\nShow the code\n#pass random people walking around the 5Cs\nstart_walking_for &lt;- function(number_of_minutes, how_close){\n  people_passed &lt;- map_dbl(1:number_of_minutes, function(x){\n    round(rexp(1, exprate))}) #vector of the number of people I pass every minute\n  list_of_people &lt;- map(people_passed, function(x){round(runif(x, min = 1, max = 6000))})\n  vector_of_people &lt;- unlist(list_of_people) #Time steps do not matter, so it doesn't matter if I see someone in minute 1 or minute 156.\n  sum(vector_of_people &lt;= how_close) #How many times do I see someone who is my close friend?\n}\n\n\nTo count the number of times I see my close friends, I have to set a cutoff for what counts as a close friend and what counts as a not-so-close friend. I arbitrarily picked that number as 15, since I feel like there are about 15 people in the 5Cs who I would stop and have a conversation with in the middle of the night if I saw them.\nI ran this simulation for 5000 nights to see what the average number of friends I would see per 180-minute night would be.\n\n\nShow the code\nwalking_time &lt;- 180\nfriends &lt;- 15\nnights &lt;- 5000  \n\nsimulation &lt;- data.frame(friends_seen = map_dbl(1:nights, ~start_walking_for(walking_time, friends)))\n\nggplot(simulation) +\n  geom_histogram(aes(x = friends_seen), binwidth = 0.5) +\n  labs(\n    title = \"How many times do I see a certain amount of friends in one night?\",\n    subtitle = paste(\"Total nights = \", nights),\n    x = \"Number of friends seen\",\n    y = \"Number of nights walking around\"\n  )\n\n\n\n\n\n\n\n\n\nThis plot shows that it is very unlikely to have a night like my Saturday night where I saw 8 of my close friends. This means that either I am more likely to see my friends than non-friends (which is true since I knew where they generally were that night and used Find My once to find a friend), that I saw more people on my walk than I estimated, or that I am closer with more people than I estimated.\nThe plot also shows that I am most likely to see one friend on a given 3-hour walk, but for about 900 of my 5000 simulated walks, I see no friends.\nTo simulate my random night walks around the 5Cs, I generated randomly sized groups of people that I saw every minute that usually ranged from 0 to around 18 people. Then, I gave each person in that group a random “closeness score” to me, which indicated how well I knew them. Then, I counted how many times I ran into people that were a certain degree of closeness to me, and ran that simulation many times to determine what the most likely outcome of a night of randomly walking around the 5C campus was (in places where other people also go).\nOne way that this model could be improved is to account for my arrival to points of interest where many people congregate at once like Jay’s Place, the Hub, or parties. Another way that this simulation could be improved is to account for the fact that most of my friends are friends with each other, so they are more likely to be traveling in a group together than dispersed among many groups of people."
  },
  {
    "objectID": "SQL.html",
    "href": "SQL.html",
    "title": "SQL",
    "section": "",
    "text": "In this project, I want to see if race plays a factor in whether people get arrested, a citation, or a warning when they are stopped. I plan to compare the proportion of stops that end in citations and arrests across the recorded races in San Francisco, Oakland, and San Jose. The end goal of this project is to show a table with the percentages of traffic stops in each city that end in arrests, citations, and warnings and compare the data across races. I ask this question because I am curious what data can show about race playing a role in the outcome of traffic stops, since racial bias can often play a role in these outcomes.\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(RMariaDB)\nlibrary(DBI)\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\n\na reference / documentation of the data source (see below, make sure to include the citation!)\n\n\n\nShow the code\nSHOW tables\n\n\nSan Francisco:\n\n\nShow the code\nSELECT outcome, COUNT(*) AS n\nFROM ca_san_francisco_2020_04_01\nGROUP BY outcome;\n\n\n\n4 records\n\n\noutcome\nn\n\n\n\n\nNA\n15681\n\n\narrest\n11925\n\n\ncitation\n631602\n\n\nwarning\n245862\n\n\n\n\n\n\n\nShow the code\nSELECT citable.subject_race AS 'Race of Subject',\n        total.number_stops AS 'Total Stops',\n        citable.number_citations / total.number_stops * 100 AS 'Citations %',\n        warntable.number_warnings / total.number_stops * 100 AS 'Warnings %',\n        arrest.number_arrest / total.number_stops * 100 AS 'Arrests %',\n        na.null_stops / total.number_stops * 100 AS 'Other Outcome %'\n        \nFROM(\nSELECT subject_race,\nCOUNT(*) AS number_citations\nFROM ca_san_francisco_2020_04_01 \nWHERE outcome = 'citation'\nGROUP BY subject_race\n) AS citable\n\nJOIN \n\n(SELECT subject_race,\nCOUNT(*) AS number_warnings\nFROM ca_san_francisco_2020_04_01\nWHERE outcome = 'warning'\nGROUP BY subject_race) AS warntable ON citable.subject_race = warntable.subject_race\n\nJOIN\n\n(SELECT subject_race,\nCOUNT(*) AS number_arrest\nFROM ca_san_francisco_2020_04_01\nWHERE outcome != 'warning' AND outcome != 'citation'\nGROUP BY subject_race) AS arrest ON citable.subject_race = arrest.subject_race\n\nJOIN\n\n(SELECT subject_race,\nCOUNT(*) AS number_stops\nFROM ca_san_francisco_2020_04_01\nGROUP BY subject_race) AS total ON citable.subject_race = total.subject_race\n\nJOIN\n\n(SELECT subject_race,\nCOUNT(*) AS null_stops\nFROM ca_san_francisco_2020_04_01\nWHERE outcome IS NULL\nGROUP BY subject_race) AS na ON citable.subject_race = na.subject_race\n\nORDER BY citable.number_citations DESC;\n\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\nRace of Subject\nTotal Stops\nCitations %\nWarnings %\nArrests %\nOther Outcome %\n\n\n\n\nwhite\n372318\n71.3664\n26.5410\n1.0663\n1.0263\n\n\nasian/pacific islander\n157684\n78.2984\n20.3185\n0.7743\n0.6088\n\n\nblack\n152196\n54.9522\n39.9117\n2.3259\n2.8102\n\n\nother\n106858\n76.2114\n21.3648\n0.8516\n1.5722\n\n\nhispanic\n116014\n66.6773\n27.0933\n1.9687\n4.2607\n\n\n\n\n\nEach row in this table (and the following 2 tables) corresponds to a racial group of people that have been stopped by the police. The columns tell you what percentage of the traffic stops conducted on each race of people have ended in citations, arrests, warnings, or any other outcome. The racial groups in all three tables are ordered by the total number of stops made to people in that group during the dataset’s timeframe.\nThis table shows that in San Francisco, black and Hispanic people have a higher rate of arrest, 2.3% and 2.0% respectively from a traffic stop than white, Asian, and people who do not fit these racial categories. It also shows that black people have a higher rate of receiving warnings than the other races at 40% compared to 27%, 20%, 21%, and 27%. It seems like Asian/Pacific Islander people, white people, and people who do not fit the given racial categories have the highest rates of citation in San Francisco.\nOakland:\n\n\nShow the code\nSELECT outcome, COUNT(*) AS n\nFROM ca_oakland_2020_04_01\nGROUP BY outcome;\n\n\n\n\nShow the code\nSELECT citable.subject_race AS 'Race of Subject',\n        total.number_stops AS 'Total Stops',\n        citable.number_citations / total.number_stops * 100 AS 'Citations %',\n        warntable.number_warnings / total.number_stops * 100 AS 'Warnings %',\n        arrest.number_arrest / total.number_stops * 100 AS 'Arrests %',\n        na.null_stops / total.number_stops * 100 AS 'Other Outcome %'\n        \nFROM(\nSELECT subject_race,\nCOUNT(*) AS number_citations\nFROM ca_oakland_2020_04_01 \nWHERE outcome = 'citation'\nGROUP BY subject_race\n) AS citable\n\nJOIN \n\n(SELECT subject_race,\nCOUNT(*) AS number_warnings\nFROM ca_oakland_2020_04_01\nWHERE outcome = 'warning'\nGROUP BY subject_race) AS warntable ON citable.subject_race = warntable.subject_race\n\nJOIN\n\n(SELECT subject_race,\nCOUNT(*) AS number_arrest\nFROM ca_oakland_2020_04_01\nWHERE outcome != 'warning' AND outcome != 'citation'\nGROUP BY subject_race) AS arrest ON citable.subject_race = arrest.subject_race\n\nJOIN\n\n(SELECT subject_race,\nCOUNT(*) AS number_stops\nFROM ca_oakland_2020_04_01\nGROUP BY subject_race) AS total ON citable.subject_race = total.subject_race\n\nJOIN\n\n(SELECT subject_race,\nCOUNT(*) AS null_stops\nFROM ca_oakland_2020_04_01\nWHERE outcome IS NULL\nGROUP BY subject_race) AS na ON citable.subject_race = na.subject_race\n\nORDER BY citable.number_citations DESC \n;\n\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\nRace of Subject\nTotal Stops\nCitations %\nWarnings %\nArrests %\nOther Outcome %\n\n\n\n\nblack\n78925\n33.0922\n24.3231\n14.4973\n28.0874\n\n\nhispanic\n26257\n48.3033\n20.7830\n9.9402\n20.9735\n\n\nwhite\n15628\n46.7110\n21.6214\n7.7041\n23.9634\n\n\nasian/pacific islander\n8099\n49.3765\n21.9533\n7.5688\n21.1014\n\n\nother\n4498\n51.7119\n19.4086\n7.1365\n21.7430\n\n\n\n\n\nIn Oakland, Hispanic, white, Asian/Pacific Islander, and people who do not fit the racial categories have the highest rates of citation at 48%, 47%, 49%, and 51% respectively, although the number of “other” people in this dataset is relatively small. Black people have a higher rate of receiving warnings from traffic stops than the other races, and a significantly higher rate of being arrested.\nSan Jose:\n\n\nShow the code\nSELECT outcome, COUNT(*) AS n\nFROM ca_san_jose_2020_04_01\nGROUP BY outcome;\n\n\n\n\nShow the code\nSELECT citable.subject_race  AS 'Race of Subject',\n        total.number_stops AS 'Total Stops',\n        citable.number_citations / total.number_stops * 100 AS 'Citations %',\n        arrest.number_arrest / total.number_stops * 100 AS 'Arrests %',\n        na.null_stops / total.number_stops * 100 AS 'Other Outcome %'\n        \nFROM(\nSELECT subject_race,\nCOUNT(*) AS number_citations\nFROM ca_san_jose_2020_04_01\nWHERE outcome = 'citation'\nGROUP BY subject_race\n) AS citable\n\nJOIN\n\n(SELECT subject_race,\nCOUNT(*) AS number_arrest\nFROM ca_san_jose_2020_04_01\nWHERE outcome = 'arrest' \nGROUP BY subject_race) AS arrest ON citable.subject_race = arrest.subject_race\n\nJOIN\n\n(SELECT subject_race,\nCOUNT(*) AS number_stops\nFROM ca_san_jose_2020_04_01\nGROUP BY subject_race) AS total ON citable.subject_race = total.subject_race\n\nJOIN\n\n(SELECT subject_race,\nCOUNT(*) AS null_stops\nFROM ca_san_jose_2020_04_01\nWHERE outcome IS NULL\nGROUP BY subject_race) AS na ON citable.subject_race = na.subject_race\n\nORDER BY citable.number_citations DESC;\n\n\n\n5 records\n\n\n\n\n\n\n\n\n\nRace of Subject\nTotal Stops\nCitations %\nArrests %\nOther Outcome %\n\n\n\n\nhispanic\n79885\n26.9613\n9.2395\n63.7992\n\n\nwhite\n26341\n31.2706\n6.5639\n62.1654\n\n\nasian/pacific islander\n16062\n41.8752\n4.3892\n53.7355\n\n\nother\n11523\n42.6278\n6.2918\n51.0804\n\n\nblack\n13538\n23.6815\n9.8537\n66.4648\n\n\n\n\n\nSan Jose did not keep track of warnings, so all of their warning data has probably been aggregated with the “Other Outcome %” category. In San Jose, Hispanic and black people have the highest rate of arrest, and Asian/Pacific Islander and “other” people have the highest rate of citation.\n\n\nShow the code\ndbDisconnect(con_traffic, shutdown = TRUE)\n\n\nTo achieve my goals of showing the number of stops that ended with citations, I first checked the “outcome” variable in the table that I was working with to see what outcomes it counted. Then, I counted the number of citations, arrests, warnings (which were only counted in San Francisco and Oakland), and N/A values for each race in each city and divided them by the total number of stops in each city. Then, I joined the tables by the race of the person who was stopped so that all of the information that I just counted lived in the same table.\nAcross all three Bay Area cities, black and Hispanic people had the highest rate of arrest from a traffic stop. This could possibly indicate racial bias in the way that those racial groups are policed, and how officers go about conducting traffic stops with these people."
  },
  {
    "objectID": "Ethics.html",
    "href": "Ethics.html",
    "title": "Ethics",
    "section": "",
    "text": "Describe the example / scenario as if to someone who is not at all familiar with the setting. In particular, it should be clear both what is the data science component and what is the ethical dilemma.\nPriyanga Gunarathne, Huaxia Rui and Abraham Seidmann published a paper that claimed that airline social media representatives respond less to Twitter users when they can see that their profile picture depicts a black person. Their methods involved taking all Tweets from 2014 to 2015 that mentioned any major US airline, running the Tweets’ profile pictures through an algorithm that returned either “no face”, or tells you what race each face is, creating and training a new algorithm that predicts people’s race based on their tweets, and measuring the response rate of customer service agents depending on the race depicted in the profile is or the race predicted by all of the past tweets. The authors found that people with profile pictures that had a black person in them were 12% less likely to receive customer service through Twitter than people who had other races in their profile picture. They also found that when the person did not have a profile picture, their race, as detected by their tweet-scraping algorithm, did not have an effect on the rate at which people received customer service through Twitter. Creating the algorithm that predicts people’s race based on their tweets creates an ethical dilemma, even though the authors of this study were using the algorithm to point out an explicit case of business-to-customer racism. Algorithms like these could also be used to discriminate against people or show people different prices for items based on their social media activity. The study concluded by suggesting that social media customer service teams hide people’s profile pictures when they are doing their jobs.\nRespond to at least 4 of the items below (from the list of questions or the Data Values and Principles Manifesto). Four separate paragraphs that explain both the issue (e.g., consent) and how the issue played out in the data science example. Note: totally fine if there are items that were done well in your example.\n\nWhat is the permission structure for using the data? Was it followed?\nIt is important that anyone using data from online follows the correct permission structures for collecting and publishing the data. Permission structures for using data are in place because they give everyone who is implicated in the data the privacy rights they believe they have when they publish data to a website. The authors collected data from Twitter from 2014 and 2015. Twitter’s 2014 Terms of Service document states that anyone who wants to use data from Twitter has to access it through Twitter’s API, and does not lay out any clear guidelines for what data can and cannot be used. Therefore, if the data was collected through Twitter’s API, then the authors did not break Twitter’s permission structure for collecting data. Furthermore, Twitter’s 2014 Terms of Service state that anything that you post on Twitter is “available to the rest of the world” (Twitter 2014). However, by the time this paper was submitted to be published, Twitter’s End User License Agreement stated that it was prohibited to “utilize the Twitter Content to derive or obtain non-public information of individual Twitter users” (Twitter 2020). This is exactly what these authors did by inferring people’s races by running their tweets through an algorithm.\nWhat was the data collection process? Were the observations collected ethically? Are there missing observations?\nIn this study, the authors collected publically available Tweet and profile picture data from Twitter. The authors do not mention how they got this data, but if they were following Twitter’s terms of service, they would have had to get it using Twitter’s API. The way that these people collected this data could be seen as ethical since by posting to Twitter and making a Twitter account, any user is expected to know that anything they put there will be able to be seen by the entire world. In general, it is important to ethically collect data, since many people could be putting personal information into places and not know that it will be used for data analysis purposes. For example, collecting data from someone with the purposes of using it for a raffle and then using that data to create an email list could be problematic since the people who signed up with their personal information did not know they were signing up for promotional emails.\nIs the data identifiable? All of it? Some of it? In what way? Are the data sufficiently anonymized or old to be free of ethical concerns? Is anonymity guaranteed?\nThe study did not publish anyone’s Twitter profiles or names, so I would say it is sufficiently anonymized. In this case, anonymizing data is helpful since it does not directly tie anyone’s travel patterns, calculated race, or any other personal information to this study or to any complaints they have made on Twitter. On Twitter, anonymity is not necessarily guaranteed, but people should be free to post on Twitter without fear of their tweets being used to create a racial profile of them. In general, anonymizing data is important because it makes sure that people do not see direct personal damage or harm from any data that is being published.\nShould race be used as a variable? Is it a proxy for something else (e.g., amount of melanin in the skin, stress of navigating microaggressions, zip-code, etc.)? What about gender?\nThe issue of using race as a variable is an interesting one to consider, but since it effects many people’s day-to-day lives, I think it should be used as a variable. In this study, it was important that race was being used as a variable since the study proved that US airlines’ social media customer service teams were discriminating against black people. Generally, race should be used as a variable in studies, since it would be difficult to study racism against people if race is not being used as a variable.\n\nGiven what you described in #3 (above), summarize by explaining why it matters. Who benefits? Who is neglected or harmed? Were the ethical violations in the interest of profit? Surveillance? Power?\nThe conclusion that social media customer service agents should not be able to see people’s profile pictures when they are helping people is a good recommendation, especially since the authors found racial discrimination in the way that people were being helped. However, this conclusion could have been reached in a number of ways, many of which did not have to involve creating and training a brand-new race prediction algorithm. The authors also claimed that they trained the race prediction algorithm on 2,000 Tweets, and did not mention whether the people who were used in training the algorithm had any say over whether they were used to train this algorithm or not. This algorithm could possibly be used in the future to predict people’s races from their tweets and then use that prediction for nefarious purposes. The data and algorithm in this case were used to benefit black people and stop racial discrimination by companies. However, the authors should have been aware of the fact that creating a new race prediction algorithm for the sake of academic rigor has its drawbacks and could take away from marginalized communities.\n\nCitations:\nGina Mantica. “Racial Discrimination of Airline Customers on Twitter.” Boston University, 10 Nov. 2021, https://www.bu.edu/hic/2021/11/10/racial-discrimination-of-airline-customers-on-twitter/.\nPriyanga Gunarathne, et al. “Racial Bias in Customer Service: Evidence from Twitter.” Information Systems Research, vol. 33, no. 1, Mar. 2022, pp. 43–54.\nTwitter. “Terms of Service.” 8 Sep. 2014, https://x.com/en/tos/previous/version_8.\n———. “Twitter Content End User License Agreement.” 1 Mar. 2020, https://prod2-sprcdn-assets.sprinklr.com/50400/ce9f9f63-7d51-490d-b385-cc3349d32d9f-1123928880/Twitter_EULA_3.2020__Oct_2024_.pdf."
  },
  {
    "objectID": "Water_Insecurity.html",
    "href": "Water_Insecurity.html",
    "title": "Water Insecurity",
    "section": "",
    "text": "water_insecurity_2022 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-28/water_insecurity_2022.csv')\nwater_insecurity_2023 &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-28/water_insecurity_2023.csv')\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\nwater_insecurity &lt;- water_insecurity_2023 |&gt;\n  head(848)|&gt;\n  mutate(\"old_percent_lacking_plumbing\" = pull(select(water_insecurity_2022, percent_lacking_plumbing))) |&gt;\n  mutate(\"2022\" = mean(old_percent_lacking_plumbing, na.rm = TRUE), \"2023\" = mean(percent_lacking_plumbing, na.rm = TRUE))|&gt;\n  head(1)|&gt;\n  pivot_longer(c(\"2022\", \"2023\"),names_to = \"yeare\", values_to = \"Average_percent_lacking_plumbing\") |&gt;\n  select(yeare, Average_percent_lacking_plumbing)\nggplot(water_insecurity, aes(x = yeare, y = Average_percent_lacking_plumbing)) +\n  geom_col() +\n  labs(\n    x = \"Year\",\n    y = \"Percent Without Plumbing across the US\",\n    title = \"(Not-so-honest) Plot of the percentage of houses without full \\nplumbing in 2022 and 2023\"\n  )\n\n\n\n\n\n\n\n\nCitation:\nElmera Azadpour and Cee Nell. Mapping Water Insecurity in R with Tidycensus. 9 Dec. 2024, https://waterdata.usgs.gov/blog/acs-maps/."
  },
  {
    "objectID": "CDC_Data.html",
    "href": "CDC_Data.html",
    "title": "CDC Data",
    "section": "",
    "text": "dc_datasets &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-11/cdc_datasets.csv')\nfpi_codes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-11/fpi_codes.csv')\nomb_codes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-02-11/omb_codes.csv')\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\ndc_datasets |&gt;\n  mutate(contact_name = fct_infreq(contact_name))|&gt;\n  mutate(contact_name_other = fct_lump(contact_name, 5)) |&gt;\n  ggplot(aes(x = contact_name_other, fill = contact_name_other)) +\n  geom_bar() +\n  theme(axis.text.x = element_blank())+\n  labs(\n    title = \"Most Common Contact Name for Datasets\",\n    x = \"\",\n    y = \"Count\",\n    fill = \"Contact name\"\n  )\n\n\n\n\n\n\n\n\nCitation:\n“Files for 20250128-Cdc-Datasets.” Internet Archive, https://archive.org/download/20250128-cdc-datasets."
  }
]